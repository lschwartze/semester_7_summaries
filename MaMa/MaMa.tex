\documentclass[a4paper, 12pt]{article}

\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb}
\usepackage[explicit]{titlesec}
\usepackage{ulem}
\usepackage[onehalfspacing]{setspace}
\usepackage{amsthm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[subsection] % reset theorem numbering for each chapter

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition} % definition numbers are dependent on theorem numbers
\theoremstyle{lemma}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{example}
\newtheorem{example}[theorem]{Example}

\titleformat{\subsection}
{\small}{\thesubsection}{1em}{\uline{#1}}
\begin{document}
	\begin{titlepage} 
		\title{MaMa Summary}
		\clearpage\maketitle
		\thispagestyle{empty}
	\end{titlepage}
	\tableofcontents
	\newpage
\section{Classification Learning}
	\subsection{vocabulary}
	\begin{itemize}
		\item domain set $X$: set of possible inputs
		\item classes or label set $Y$: target of classification
		\item data points or samples: $x \in X$
		\item features or attributes: entries of $x$
		\item training set $S \subseteq X$: pairs $(x,y)$ of data points $x$ and labels $y$
		\item classifier $h$: function $X \to Y$
	\end{itemize}

	\subsection{loss and error}
	Let $h$ be a classifier $h: X \to Y$.\\
	\begin{itemize}
		\item loss function: $l(y,y') \geq 0$ for $y,y' \in Y$.
		\item zero-one loss: \[l_{0-1}(y,y') = \begin{cases}
			0, \text{ if } y = y'\\
			1, \text{ if } y \neq y'
		\end{cases}\]
	\item training error: \[L_S(h) = \frac{1}{\left|S\right|} \sum_{(x,y) \in S} l(y,h(x))\]
	\end{itemize}

	\begin{example}
		Let $X = \mathbb{R}^2$ and $Y = \{-1,1\}$. The easiest function is a linear classifier that splits $X$ in two parts and characterizes data points accordingly. For the general case of $X = \mathbb{R}^n$, $w \in \mathbb{R}^n$ and $b \in \mathbb{R}$ we have:
		\[h_{w,b} = sgn(w^Tx + b)\] 
		You can get rid of the bias $b$ by adapting the training set a little: \[\overline{S} = (\binom{x}{1},y): (x,y) \in S\]
		where the new linear classifier is defined by \[\overline{h}_{\overline{w}}(x) = sgn(\overline{w}^T\overline{x})\] with $\overline{w} = \binom{w}{b}$.
	\end{example}
	\noindent\underline{When is it possible to achieve training error 0?} (wrt zero-one-loss)\\
	This is precisely then the case, when $\exists w \in \mathbb{R}^n$ s.t. $\forall (x,y) \in S$ \begin{itemize}
		\item if $y = 1$ then $w^Tx \geq 0$ $\leftrightarrow$ if $y = 1$ then $w^Tx > 0$
		\item if $y = -1$ then $w^Tx < 0$
	\end{itemize} 
	Both can be generalized by saying \[y\cdot w^Tx > 0\]
	or \[\exists w: y \cdot w^Tx \geq 1 \; \forall (x,y) \in S\] 
	A training set with 0 training error is called separable.
	
	\subsection{logistic regression}
	Logistic regression computes a linear classifier.\\
	\underline{direct way:}\\
	Look for a $w \in \mathbb{R}^n$ such that the training error is minimised, i.e. \[\min\limits_{w \in \mathbb{R}^n} \frac{1}{\left|S\right|} \sum_{(x,y) \in S} h_{0-1}(y,h_w(x))\]
	
	\begin{definition}[logistic function]
		The logistic function $\phi_{sig}: \mathbb{R} \to [0,1]$ is defined by \[z \mapsto \frac{1}{1+e^{-z}}\]
		We then solve \[\min\limits_{w \in \mathbb{R}^n} \frac{1}{\left|S\right|} \sum_{(x,y) \in S} - \log_2(\phi_{sig}(yw^Tx))\]
	\end{definition}
	
	\begin{lemma}
		For all training sets $S \subseteq \mathbb{R}^n \times \{-1,1\}$ it holds that \[\frac{1}{\left|S\right|} \sum_{(x,y) \in S} h_{0-1}(y,h_w(x)) \leq \frac{1}{\left|S\right|} \sum_{(x,y) \in S} -log_2(\phi_{sig}(yw^Tx))\]
	\end{lemma}

	\subsection{training error and real life}
	A classifier is only good, if it performs good on new data. To test your classifier you need to evaluate the classifier on data it hasn't seen during training. This is called the test error. The question now is, how the data should be split into test and training data. This is a difficult question and should be evaluated for each use case separately. A part from the training set should also be used for validation if there are degrees of freedom in the chosen classifier.
	
	\subsection{Quadratic classifier}
	This is another binary classifier. In this case wa search for a matrix $U \in \mathbb{R}^{n\times n}$ of weights, a vector $w \in \mathbb{R}^n$ and a bias $b \in \mathbb{R}$. The classifier is then defined by \[h(x) = sgn\left(\sum_{i,j = 1}^{n} u_{i,j}x_ix_j + \sum_{i=1}^{n} w_ix_i + b\right) = sgn(x^Tux + w^Tx + b)\]
	To simplify things we redefine the training set $S$ by $\overline{S}$ containing all samples $(x,y)$ with $x = \begin{pmatrix}
		x_1\\
		\vdots\\
		x_n
	\end{pmatrix}$ and $y$ analogously defined. Redefine $x$ by \[\overline{x} = (x_1^2, x_1x_2, x_1x_3, .., x_n^2,.., x_n,1)\]
	Doing this will transform the quadratic classifier into a linear one.
	
	\subsection{nearest neighbour classifier}
	The training set again is defined by $S \subseteq X\times Y$. When adding a new data-point, we check what class the nearest data point belongs to and add the new point to the same class. In the case of $k$-nearest-neighbour we check the classes of the $k$ nearest points. The problem is that the training set must be in memory the entire time. This makes the learning potentially very slow.
	
	\subsection{decision tree classifier}
	The classifier of a decision tree works as follows: \begin{enumerate}
		\item Set $v = r$ where $r$ is the tree's root
		\item while $v$ is not a leaf do
		\item Let $(i,j)$ be the decision rule of $v$
		\item If $x_i \leq t$ set $v = v_L$ else set $v = v_R$
		\item end while
		\item Output the class $c(v)$ of $v$
	\end{enumerate}
	The following now describes how a tree can be learned.\\
	The idea is to start with a single node $r$ where the class $c(r)$ is just the majority class in $S$.\\
	Iteratively decide for each leaf $v$, if it is beneficial to split it into two child nodes. Let $S_v$ be the path of the training set that, following the existing decisions, ends up in $v$. The split into $S_L$ and $S_R$ would then be defined by a feature $i$ and a threshold $t$. We define the gain of the split by \[gain(v,i,t) = \gamma(S_v) - \left(\frac{\left|S_L\right|}{\left|S_R\right|}\gamma(S_L) + \frac{\left|S_R\right|}{\left|S_L\right|}\gamma(S_R)\right)\] where $\gamma$ is a inhomogeneity measure. This can be defined in multiple ways.
	\begin{definition}
		The inhomogeneity can be defined by the training error \[\gamma(S_v) = 1- \max\limits_{y \in Y} p(y,S_v)\] where \[p(y,S_v) = \frac{\left|\{(x,y') \in S_v: \; y' = y\}\right|}{\left|S_v\right|}\]
	\end{definition}
	\begin{definition}[gini impurity]
		This is used in scikit-learn. \[\gamma(S_v) = 1- \sum_{y \in Y} p(y, S_v)^2\]
	\end{definition}
	\begin{definition}[entropy]
		\[\gamma(S_v) = -\sum_{y \in Y} p(y,S_v)\cdot\log_2(y,S_v)\]
	\end{definition}
	\subsection{loss functions}
	So far, we only looked at the zero-one-loss $l_{0-1}(y,y') = \begin{cases}
		1, \; y \neq y'\\
		0, \; y = y'
	\end{cases}$ where $y$ is the true class and $y'$ is the prediction.\\
	Consider the use case of a spam filter. This is a binary classifier that checks whether a new mail is spam or not. This admits two kind of errors: \begin{itemize}
		\item false-positive: good mail is classified as spam
		\item false-negative: spam mail is classified as good
	\end{itemize}
	The case of false-positive is more serious in this example. This needs to be applied to the loss function. \[l(y,y') = \begin{cases}
		0, \; y=y'\\
		10, \; \text{if $y$ is good and $y'$ spam}\\
		1, \; \text{if $y$ is spam and $y'$ is good}
	\end{cases}\]
	\begin{example}[loss functions in regressions]
		Consider a predictor $h: \mathbb{R}^n \to \mathbb{R}$. The (mean)square-loss is defined by \[l(y,y') = (y-y')^2\] and the (mean) absolute loss \[l(y,y') = \left|y-y'\right|\]
	\end{example}
	\subsection{A statistical model}
	Let $X$ and $Y$ be sets as above. We define a probability distribution $D$ on $X\times Y$ with the following assumptions \begin{itemize}
		\item $D$ is unknown
		\item if $D$ were defined on $X$ only, then $\mathbb{P}[p] = \begin{cases}
			\text{large if picture shows cat or dog}\\
			\text{small if not}
		\end{cases}$ where $p$ is a picture. Instead $D$ is defined on $X\times Y$ because there might be some uncertainty in $X$\\
	\item iid: data points of the training set are drawn from $D$ independently
	\item $D$ is fixed 
	\end{itemize}
	A classifier $h^*: X \to Y$ works well on new data if \[L_D(h^*) = \mathbb{E}_{(x,y)\sim D}[l(y,h^*(x))]\] is small. This is called true risk or generalization error. For classification using the zero-one-loss this simplifies to \[L_D(h^*) = \mathbb{P}_{(x,y)\sim D}[h^*(x)\neq y]\]
	\subsection{Bayes error and classifier}
	The Bayes error is the smallest error that any classifier can achieve.
	\[\varepsilon_{bayes} = \inf\limits_{h} L_D(h)\] A Bayes classifier $h^*$ such that $L_D(h^*) = \varepsilon_{bayes}$ is called a Bayes classifier.
	\begin{theorem}
		For all classifiers $h: X \to Y$ it holds that \[L_D(h) \geq L_D(h_{bayes})\]
	\end{theorem}

	\section{PAC learning}
	Let $H$ be a set of classifiers. Draw a training set $S$ from $D$. The goal is to minimize $h_S = \arg\min\limits_{h \in H}L_S(h)$. This is called the method of empirical risk minimization.
	\subsection{empirical risk minimisation}
	\begin{lemma}[Hoeffding's inequality]
		Given independent random variables $X_1,...,X_m: \Omega \to [a,b]$ with $\mathbb{E}[X_i] = \mu$ for all $i\in[n]$.
		Then $$\mathbb{P}\left[\left|\frac{1}{m}\sum_{i=1}^m X_i-\mu\right|\right] \leq 2\cdot \exp\left(-2\frac{m\varepsilon^2}{(b-a)^2}\right)$$
	\end{lemma}
	We now want to figure out the connection between $L_S(h)$ and $L_D(H)$. Let $\left|S\right| = m$ and $S = \{(x_1,y_1), ... (x_m,y_m)\}$. Define $X_i = l(y_i,h(x_i))$. Then the training error is given by \[\frac{1}{m} = \sum_{i=1}^m X_i = L_S(h)\]
	Furthermore, $\mathbb{E}[X_i] = \mathbb{E}_{(x,y) \sim D}[l(y,h(x))] = L_D(h)$. Assuming zero-one-loss, Hoeffing implies \[\mathbb{P}[\left|L_S(h) - L_D(h)\right| \geq \varepsilon] \leq 2\cdot \exp(-2m\varepsilon^2) := \delta\]
	Then $\varepsilon = \sqrt{\frac{\ln(\frac{2}{\delta})}{2m}}$
	\begin{theorem}
		With probability $\geq 1-\delta$ it holds that \[\left|L_S(h) - L_D(h)\right| \leq \sqrt{\frac{\ln(\frac{2}{\delta})}{2m}}\]
	\end{theorem}
	This does not however imply that with high probability $L_S(h_S)\approx L_D(h_S)$. It is however true for the test-set: with probability $1-\delta$ it holds that $\left|L_T(h_S)-L_D(h_S)\right| \leq \sqrt{\frac{\ln(\frac{2}{\delta})}{2\left|T\right|}}$.
	
	\subsection{error decomposition}
	Assume we have a training set $S$ and a test set $T$ and the training error $h_S$ has been minimised by empirical risk minimisation. We are interested in the generalisation error $L_D(h_s)$. For example assume a training error of $9\%$ and a test error of $12\%$. We can decompose the unknown generalisation error as follows: \[L_D(h_S) = \underset{\text{small if $T$ reasonably large}}{(L_D(h_S) - L_T(h_S))} + \underset{\text{generalisation gap,} 12\%-9\%}{(L_T(h_S)-L_S(h_S))} + \underset{9\%}{L_S(h_S)}\]
	
	A large generalisation gap means that the classifier learns the training set and not the underlying distribution $D$. That means the classifier is overfitting.
	\subsection{finitely many classifiers}
	\begin{lemma}
		Let $H$ be a set of classifiers. Assume that for every $\varepsilon, \delta > 0$, there is an $m$ s.t. when drawing a sample $S$ of size at least $m$ then, with a probability at least $1-\delta$ it holds that \[\sup_{h \in H} \left|L_D(h) - LS_(H)\right| \leq \frac{\varepsilon}{2}\] Then with probability at least $1-\delta$ it holds that \[L_D(h_S) \leq \inf_{h \in H}L_D(h)+\varepsilon\]
		These assumptions are called the uniform convergence property.
	\end{lemma}
	\begin{theorem}
		Let $\varepsilon, \delta > 0$ and $H$ be a finite class of classifiers. Let the training set $S$ have size $m\geq \frac{2}{\varepsilon^2}\ln\frac{2\left|H\right|}{\delta}$. Then with probability at least $1-\delta$ it holds that \[L_D(h_S) \leq \min_{h \in H} L_D(h) + \varepsilon\]
		This proves that empirical risk minimisation works.
	\end{theorem}
	\subsection{probably approximately correct (pac)}
	Let $H$ be a set of classifiers. $H$ is agnostically PAC learnable if\\
	(informal) there is a learning algorithm that provided the training set is large enough, returns an almost optimal classifier.\\
	(formal) $\exists m_H: (0,1)^2 \to \mathbb{Z}_+$ and a learning algorithm $A$ (think risk minimisation) s.t. $\forall \varepsilon,\delta \in (0,1)$ and $\forall$ probability distributions $D$ on $X\times Y$ if $S$ with $\left|S\right|\geq m_H(\varepsilon, \delta)$ is iid drawn from $D$ then with probability $\geq 1-\delta$ it holds that \[L_D(A(S)) \leq \inf_{h \in H}L_D(h)+\varepsilon\]
	Classes $H$ that are finite are PAC-learnable. Also classes that satisfy the uniform convergence property are PAC-learnable as well.
	\subsection{VC-dimension}
	Very informally this dimension gives an idea of how powerful a class of classifiers can be.\\
	The VC-dimension is only defined for binary classifiers, say with classes $0$ and $1$. Let $H$ be a set of binary classifiers (for example axis-parallel rectangle classifiers, H = $\{I_R: R \text{ axis parallel rectangle}\}$, so $I_R(x) = \begin{cases}
		1, x \in R\\
		0, x \notin R
	\end{cases}$).
	Formal definition: We say that $H$ shatters a set $C \subset X$ if \[\{h|_C: h \in H\} = H|_C \overset{!}{=} \{f: C \to \{0,1\}\} \Leftrightarrow \left|H|_C\right| = 2^{\left|C\right|}\]  
	The VC-dimension of $H$ is then the largest $d$ s.t. \[\exists C \subseteq X \text{ of } \left|C\right| = d \text{ s.t. } C \text{ is shattered by } H\]
	The dimension of $H$ is infinite if $\forall k \in \mathbb{Z}_+ \; \exists C \subseteq X$ of $\left|C\right| = X$ that is shattered by $H$.\\
	It is easy to see that the VC-dimension of the axis parallel rectangle classifier is at least 4 and in fact it is exactly for as we will see below.\\
	Consider any set of 5 points in $\mathbb{R}^2$. Let $p_1$ be the point with the largest $y$-coordinate and $p_2$ be the point with the smallest $y$-coordinate. $p_3$ has the largest $x$ coordinate and $p_4$ has the smallest. Notice that each pair of points may be identical. Finally, there is a fifth point $q$ that is somewhere in the rectangle spanned by those four points. If we give points $p_1$ to $p_4$ the class 1 and $q$ class 0, then we can't shatter any set of five points implying that VC-dim $\leq 4$.
	\begin{example}
		Let $X = \mathbb{R}$. Define \[H = \{h_{[a,b]}: a\leq b\}\] where \[h_{[a,b]}(x) = \begin{cases}
			1 \; x \in [a,b]\\
			0; \text{ else}
		\end{cases}\]
		Obviously $VC(H) \geq 2$ because there is always an interval that contains two points. Indeed the dimension is exactly 2 because if we have $x_1 < x_2 < x_3$ with classes 1, 0 and 1, there is no classifier that is correct.
	\end{example}
	The class of homogeneous linear classifiers in $\mathbb{R}^d$ \[h_w: x \mapsto sgn(w^Tx)\] has VC-dimension of $d$.
	\subsection{Fundamental Theorem of PAC-learning}
	\begin{theorem}
		Let $H$ be a set of binary classifiers. Then $H$ is PAC-learnable iff the VC dimension of $H$ is finite.
	\end{theorem}
	\begin{theorem}
		Let $H$ be a set of binary classifiers with $VC(H) = d < \infty$. Then $\exists c > 0$ s.t. $\forall \varepsilon, \delta > 0$ it holds that with probability at least $1-\delta$ \[L_D(h_S) \leq \inf_{h \in H} L_D(h) + \sqrt{c\frac{d+\log(\frac{1}{\delta})}{m}}\]
	\end{theorem}
	\begin{definition}
		Let $H$ be a set of binary classifiers. We define the growth function \[\tau: \mathbb{N} \to \mathbb{N}\] with \[\tau(n) = \max\limits_{C\subseteq X, \left|C\right| = m} \left|H|_C\right| = \max\left|\{h|_c: C \to \{0,1\}: h \in H\}\right|\]
	\end{definition}
	\begin{lemma}
		Let $H$ be a set of binary classifiers with $VC(H) = d < \infty$. Then \[\tau(m) \leq \sum_{i=0}^d \binom{m}{i} \; \forall m \in \mathbb{N}\] In particular if \[m \geq d+1\] then $\tau(m) \leq (d+1)\cdot m^{d+1}$.
	\end{lemma}
	\begin{theorem}
		Let $H$ be a set of binary classifiers. Then for every $\delta >0$ with probability of at least $1-\delta$ over the choice $S\sim D^m$ of the training set it holds that \[\sup_{h \in H} \left|L_D(h) - L_S(h)\right| \leq \frac{4+\sqrt{\log(\tau_H(2\cdot m))}}{\delta\sqrt{2\cdot m}}\] 
	\end{theorem}
	\begin{lemma}
		Let $H$ be a set of binary classifiers. Then for every $\delta > 0$ with probability at least $1-\delta$ over the choice $S\sim D^m$ of the training set it holds that \[\sup_{h \in H} \left|L_D(h) - L_S(h)\right| \leq \frac{8+2\sqrt{\log(\tau_H(2\cdot m))}}{\delta \sqrt{m}}\]
	\end{lemma}
	\underline{Note:} The lemma is a weaker bound than the one in above theorem and can thus be ignored.
	\begin{theorem}
		Let $H$ be a set of binary classifiers with the domain set $X$ of VC-dimension $\geq2m$ and a learning algorithm $A$. Then there is a distribution $D$ on $X\times \{0,1\}$ s.t. \begin{enumerate}
			\item $\exists h^* \in H$ with $L_D(h^*) = 0$
			\item with probability $\geq \frac{1}{7}$ and a choice of training set $S$ with $\left|S\right| = m$ we have: \[L_D(A(S)) \geq \frac{1}{8}\]
		\end{enumerate}
	\end{theorem}

	\subsection{VC-dimension of linear classifiers}
	\begin{theorem}
		The class of linear classifiers in $\mathbb{R}^d$ \[H = \{x\mapsto sgn(w^Tx): \; w \in \mathbb{R}^d\}\]
		has VC-dimension $d$.
	\end{theorem}
	\subsection{neural networks}
	For an input vector $x$, weights $w$ and a bias $b$ the output of an artificial neuron is $w^Tx+b$.\\
	A classification network is made up of an input layer, the hidden layer where the neurons are and the output layer. In such a case, the weight vector $w$ would be a weight matrix $W^{(1)}$. The each neuron then has a weight to the output neuron which also has a bias $b^{(2)}$ so the final output is then given by $sgn(W^{(2)}h + b^{(2)})$ where $h$ is the output of the hidden layer. One can notice that this is simply an affine function. Hence neurons are augmented by an activation function, e.g. ReLU$(x) = \max(0,x)$ (=Rectified linear unit). If the activation function is non-linear one can calculate more difficult stuff than with normal affine functions.\\
	Activation functions $\sigma: \mathbb{R} \to \mathbb{R}$ are applied component-wise. It is important to note, that neurons on the same layer must share the same activation function. The necessity of activation functions becomes inherently apparent, when one tries to compute XOR with a neural network. Even though XOR is not separable, one can compute the XOR function using a neural network, if an activation function like ReLU is used.\\
	Modern neural networks consist of many hidden layers, each equipped with ReLU while the output layer usually uses the logistic function or softmax. A logistic function output does not give a simple class but a confidence level instead. This is a difference to the classical classification task. Quite similarly, one can use a softmax function defined by \[z \mapsto \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}\] This gives a distribution over the classes instead. 
		
\end{document}