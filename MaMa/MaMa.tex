\documentclass[a4paper, 12pt]{article}

\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb}
\usepackage[explicit]{titlesec}
\usepackage{ulem}
\usepackage[onehalfspacing]{setspace}
\usepackage{amsthm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[subsection] % reset theorem numbering for each chapter

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition} % definition numbers are dependent on theorem numbers
\theoremstyle{lemma}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{example}
\newtheorem{example}[theorem]{Example}

\titleformat{\subsection}
{\small}{\thesubsection}{1em}{\uline{#1}}
\begin{document}
	\begin{titlepage} 
		\title{MaMa Summary}
		\clearpage\maketitle
		\thispagestyle{empty}
	\end{titlepage}
	\tableofcontents
	\newpage
\section{Classification Learning}
	\subsection{vocabulary}
	\begin{itemize}
		\item domain set $X$: set of possible inputs
		\item classes or label set $Y$: target of classification
		\item data points or samples: $x \in X$
		\item features or attributes: entries of $x$
		\item training set $S \subseteq X$: pairs $(x,y)$ of data points $x$ and labels $y$
		\item classifier $h$: function $X \to Y$
	\end{itemize}

	\subsection{loss and error}
	Let $h$ be a classifier $h: X \to Y$.\\
	\begin{itemize}
		\item loss function: $l(y,y') \geq 0$ for $y,y' \in Y$.
		\item zero-one loss: \[l_{0-1}(y,y') = \begin{cases}
			0, \text{ if } y = y'\\
			1, \text{ if } y \neq y'
		\end{cases}\]
	\item training error: \[L_S(h) = \frac{1}{\left|S\right|} \sum_{(x,y) \in S} l(y,h(x))\]
	\end{itemize}

	\begin{example}
		Let $X = \mathbb{R}^2$ and $Y = \{-1,1\}$. The easiest function is a linear classifier that splits $X$ in two parts and characterizes data points accordingly. For the general case of $X = \mathbb{R}^n$, $w \in \mathbb{R}^n$ and $b \in \mathbb{R}$ we have:
		\[h_{w,b} = sgn(w^Tx + b)\] 
		You can get rid of the bias $b$ by adapting the training set a little: \[\overline{S} = (\binom{x}{1},y): (x,y) \in S\]
		where the new linear classifier is defined by \[\overline{h}_{\overline{w}}(x) = sgn(\overline{w}^T\overline{x})\] with $\overline{w} = \binom{w}{b}$.
	\end{example}
	\noindent\underline{When is it possible to achieve training error 0?} (wrt zero-one-loss)\\
	This is precisely then the case, when $\exists w \in \mathbb{R}^n$ s.t. $\forall (x,y) \in S$ \begin{itemize}
		\item if $y = 1$ then $w^Tx \geq 0$ $\leftrightarrow$ if $y = 1$ then $w^Tx > 0$
		\item if $y = -1$ then $w^Tx < 0$
	\end{itemize} 
	Both can be generalized by saying \[y\cdot w^Tx > 0\]
	or \[\exists w: y \cdot w^Tx \geq 1 \; \forall (x,y) \in S\] 
	A training set with 0 training error is called separable.
	
	\subsection{logistic regression}
	Logistic regression computes a linear classifier.\\
	\underline{direct way:}\\
	Look for a $w \in \mathbb{R}^n$ such that the training error is minimised, i.e. \[\min\limits_{w \in \mathbb{R}^n} \frac{1}{\left|S\right|} \sum_{(x,y) \in S} h_{0-1}(y,h_w(x))\]
	
	\begin{definition}[logistic function]
		The logistic function $\phi_{sig}: \mathbb{R} \to [0,1]$ is defined by \[z \mapsto \frac{1}{1+e^{-z}}\]
		We then solve \[\min\limits_{w \in \mathbb{R}^n} \frac{1}{\left|S\right|} \sum_{(x,y) \in S} - \log_2(\phi_{sig}(yw^Tx))\]
	\end{definition}
	
	\begin{lemma}
		For all training sets $S \subseteq \mathbb{R}^n \times \{-1,1\}$ it holds that \[\frac{1}{\left|S\right|} \sum_{(x,y) \in S} h_{0-1}(y,h_w(x)) \leq \frac{1}{\left|S\right|} \sum_{(x,y) \in S} -log_2(\phi_{sig}(yw^Tx))\]
	\end{lemma}

	\subsection{training error and real life}
	A classifier is only good, if it performs good on new data. To test your classifier you need to evaluate the classifier on data it hasn't seen during training. This is called the test error. The question now is, how the data should be split into test and training data. This is a difficult question and should be evaluated for each use case separately. A part from the training set should also be used for validation if there are degrees of freedom in the chosen classifier.
	
	\subsection{Quadratic classifier}
	This is another binary classifier. In this case wa search for a matrix $U \in \mathbb{R}^{n\times n}$ of weights, a vector $w \in \mathbb{R}^n$ and a bias $b \in \mathbb{R}$. The classifier is then defined by \[h(x) = sgn\left(\sum_{i,j = 1}^{n} u_{i,j}x_ix_j + \sum_{i=1}^{n} w_ix_i + b\right) = sgn(x^Tux + w^Tx + b)\]
	To simplify things we redefine the training set $S$ by $\overline{S}$ containing all samples $(x,y)$ with $x = \begin{pmatrix}
		x_1\\
		\vdots\\
		x_n
	\end{pmatrix}$ and $y$ analogously defined. Redefine $x$ by \[\overline{x} = (x_1^2, x_1x_2, x_1x_3, .., x_n^2,.., x_n,1)\]
	Doing this will transform the quadratic classifier into a linear one.
	
	\subsection{nearest neighbour classifier}
	The training set again is defined by $S \subseteq X\times Y$. When adding a new data-point, we check what class the nearest data point belongs to and add the new point to the same class. In the case of $k$-nearest-neighbour we check the classes of the $k$ nearest points. The problem is that the training set must be in memory the entire time. This makes the learning potentially very slow.
	
	\subsection{decision tree classifier}
	The classifier of a decision tree works as follows: \begin{enumerate}
		\item Set $v = r$ where $r$ is the tree's root
		\item while $v$ is not a leaf do
		\item Let $(i,j)$ be the decision rule of $v$
		\item If $x_i \leq t$ set $v = v_L$ else set $v = v_R$
		\item end while
		\item Output the class $c(v)$ of $v$
	\end{enumerate}
	The following now describes how a tree can be learned.\\
	The idea is to start with a single node $r$ where the class $c(r)$ is just the majority class in $S$.\\
	Iteratively decide for each leaf $v$, if it is beneficial to split it into two child nodes. Let $S_v$ be the path of the training set that, following the existing decisions, ends up in $v$. The split into $S_L$ and $S_R$ would then be defined by a feature $i$ and a threshold $t$. We define the gain of the split by \[gain(v,i,t) = \gamma(S_v) - \left(\frac{\left|S_L\right|}{\left|S_R\right|}\gamma(S_L) + \frac{\left|S_R\right|}{\left|S_L\right|}\gamma(S_R)\right)\] where $\gamma$ is a inhomogeneity measure. This can be defined in multiple ways.
	\begin{definition}
		The inhomogeneity can be defined by the training error \[\gamma(S_v) = 1- \max\limits_{y \in Y} p(y,S_v)\] where \[p(y,S_v) = \frac{\left|\{(x,y') \in S_v: \; y' = y\}\right|}{\left|S_v\right|}\]
	\end{definition}
	\begin{definition}[gini impurity]
		This is used in scikit-learn. \[\gamma(S_v) = 1- \sum_{y \in Y} p(y, S_v)^2\]
	\end{definition}
	\begin{definition}[entropy]
		\[\gamma(S_v) = -\sum_{y \in Y} p(y,S_v)\cdot\log_2(y,S_v)\]
	\end{definition}
	\subsection{loss functions}
	So far, we only looked at the zero-one-loss $l_{0-1}(y,y') = \begin{cases}
		1, \; y \neq y'\\
		0, \; y = y'
	\end{cases}$ where $y$ is the true class and $y'$ is the prediction.\\
	Consider the use case of a spam filter. This is a binary classifier that checks whether a new mail is spam or not. This admits two kind of errors: \begin{itemize}
		\item false-positive: good mail is classified as spam
		\item false-negative: spam mail is classified as good
	\end{itemize}
	The case of false-positive is more serious in this example. This needs to be applied to the loss function. \[l(y,y') = \begin{cases}
		0, \; y=y'\\
		10, \; \text{if $y$ is good and $y'$ spam}\\
		1, \; \text{if $y$ is spam and $y'$ is good}
	\end{cases}\]
	\begin{example}[loss functions in regressions]
		Consider a predictor $h: \mathbb{R}^n \to \mathbb{R}$. The (mean)square-loss is defined by \[l(y,y') = (y-y')^2\] and the (mean) absolute loss \[l(y,y') = \left|y-y'\right|\]
	\end{example}
	\subsection{A statistical model}
	Let $X$ and $Y$ be sets as above. We define a probability distribution $D$ on $X\times Y$ with the following assumptions \begin{itemize}
		\item $D$ is unknown
		\item if $D$ were defined on $X$ only, then $\mathbb{P}[p] = \begin{cases}
			\text{large if picture shows cat or dog}\\
			\text{small if not}
		\end{cases}$ where $p$ is a picture. Instead $D$ is defined on $X\times Y$ because there might be some uncertainty in $X$\\
	\item iid: data points of the training set are drawn from $D$ independently
	\item $D$ is fixed 
	\end{itemize}
	A classifier $h^*: X \to Y$ works well on new data if \[L_D(h^*) = \mathbb{E}_{(x,y)\sim D}[l(y,h^*(x))]\] is small. This is called true risk or generalization error. For classification using the zero-one-loss this simplifies to \[L_D(h^*) = \mathbb{P}_{(x,y)\sim D}[h^*(x)\neq y]\]
	\subsection{Bayes error and classifier}
	The Bayes error is the smallest error that any classifier can achieve.
	\[\varepsilon_{bayes} = \inf\limits_{h} L_D(h)\] A Bayes classifier $h^*$ such that $L_D(h^*) = \varepsilon_{bayes}$ is called a Bayes classifier.
	\begin{theorem}
		For all classifiers $h: X \to Y$ it holds that \[L_D(h) \geq L_D(h_{bayes})\]
	\end{theorem}

	\section{PAC learning}
	Let $H$ be a set of classifiers. Draw a training set $S$ from $D$. The goal is to minimize $h_S = \arg\min\limits_{h \in H}L_S(h)$. This is called the method of empirical risk minimization.
	\subsection{empirical risk minimisation}
	\begin{lemma}[Hoeffding's inequality]
		Given independent random variables $X_1,...,X_m: \Omega \to [a,b]$ with $\mathbb{E}[X_i] = \mu$ for all $i\in[n]$.
		Then $$\mathbb{P}\left[\left|\frac{1}{m}\sum_{i=1}^m X_i-\mu\right|\right] \leq 2\cdot \exp\left(-2\frac{m\varepsilon^2}{(b-a)^2}\right)$$
	\end{lemma}
	We now want to figure out the connection between $L_S(h)$ and $L_D(H)$. Let $\left|S\right| = m$ and $S = \{(x_1,y_1), ... (x_m,y_m)\}$. Define $X_i = l(y_i,h(x_i))$. Then the training error is given by \[\frac{1}{m} = \sum_{i=1}^m X_i = L_S(h)\]
	Furthermore, $\mathbb{E}[X_i] = \mathbb{E}_{(x,y) \sim D}[l(y,h(x))] = L_D(h)$. Assuming zero-one-loss, Hoeffing implies \[\mathbb{P}[\left|L_S(h) - L_D(h)\right| \geq \varepsilon] \leq 2\cdot \exp(-2m\varepsilon^2) := \delta\]
	Then $\varepsilon = \sqrt{\frac{\ln(\frac{2}{\delta})}{2m}}$
	\begin{theorem}
		With probability $\geq 1-\delta$ it holds that \[\left|L_S(h) - L_D(h)\right| \leq \sqrt{\frac{\ln(\frac{2}{\delta})}{2m}}\]
	\end{theorem}
	This does not however imply that with high probability $L_S(h_S)\approx L_D(h_S)$. It is however true for the test-set: with probability $1-\delta$ it holds that $\left|L_T(h_S)-L_D(h_S)\right| \leq \sqrt{\frac{\ln(\frac{2}{\delta})}{2\left|T\right|}}$.
	
	\subsection{error decomposition}
	Assume we have a training set $S$ and a test set $T$ and the training error $h_S$ has been minimised by empirical risk minimisation. We are interested in the generalisation error $L_D(h_s)$. For example assume a training error of $9\%$ and a test error of $12\%$. We can decompose the unknown generalisation error as follows: \[L_D(h_S) = \underset{\text{small if $T$ reasonably large}}{(L_D(h_S) - L_T(h_S))} + \underset{\text{generalisation gap,} 12\%-9\%}{(L_T(h_S)-L_S(h_S))} + \underset{9\%}{L_S(h_S)}\]
	
	A large generalisation gap means that the classifier learns the training set and not the underlying distribution $D$. That means the classifier is overfitting.
	\subsection{finitely many classifiers}
	\begin{lemma}
		Let $H$ be a set of classifiers. Assume that for every $\varepsilon, \delta > 0$, there is an $m$ s.t. when drawing a sample $S$ of size at least $m$ then, with a probability at least $1-\delta$ it holds that \[\sup_{h \in H} \left|L_D(h) - LS_(H)\right| \leq \frac{\varepsilon}{2}\] Then with probability at least $1-\delta$ it holds that \[L_D(h_S) \leq \inf_{h \in H}L_D(h)+\varepsilon\]
		These assumptions are called the uniform convergence property.
	\end{lemma}
	\begin{theorem}
		Let $\varepsilon, \delta > 0$ and $H$ be a finite class of classifiers. Let the training set $S$ have size $m\geq \frac{2}{\varepsilon^2}\ln\frac{2\left|H\right|}{\delta}$. Then with probability at least $1-\delta$ it holds that \[L_D(h_S) \leq \min_{h \in H} L_D(h) + \varepsilon\]
		This proves that empirical risk minimisation works.
	\end{theorem}
	\subsection{probably approximately correct (pac)}
	Let $H$ be a set of classifiers. $H$ is agnostically PAC learnable if\\
	(informal) there is a learning algorithm that provided the training set is large enough, returns an almost optimal classifier.\\
	(formal) $\exists m_H: (0,1)^2 \to \mathbb{Z}_+$ and a learning algorithm $A$ (think risk minimisation) s.t. $\forall \varepsilon,\delta \in (0,1)$ and $\forall$ probability distributions $D$ on $X\times Y$ if $S$ with $\left|S\right|\geq m_H(\varepsilon, \delta)$ is iid drawn from $D$ then with probability $\geq 1-\delta$ it holds that \[L_D(A(S)) \leq \inf_{h \in H}L_D(h)+\varepsilon\]
	Classes $H$ that are finite are PAC-learnable. Also classes that satisfy the uniform convergence property are PAC-learnable as well.
	\subsection{VC-dimension}
	Very informally this dimension gives an idea of how powerful a class of classifiers can be.\\
	The VC-dimension is only defined for binary classifiers, say with classes $0$ and $1$. Let $H$ be a set of binary classifiers (for example axis-parallel rectangle classifiers, H = $\{I_R: R \text{ axis parallel rectangle}\}$, so $I_R(x) = \begin{cases}
		1, x \in R\\
		0, x \notin R
	\end{cases}$).
	Formal definition: We say that $H$ shatters a set $C \subset X$ if \[\{h|_C: h \in H\} = H|_C \overset{!}{=} \{f: C \to \{0,1\}\} \Leftrightarrow \left|H|_C\right| = 2^{\left|C\right|}\]  
	The VC-dimension of $H$ is then the largest $d$ s.t. \[\exists C \subseteq X \text{ of } \left|C\right| = d \text{ s.t. } C \text{ is shattered by } H\]
	The dimension of $H$ is infinite if $\forall k \in \mathbb{Z}_+ \; \exists C \subseteq X$ of $\left|C\right| = X$ that is shattered by $H$.\\
	It is easy to see that the VC-dimension of the axis parallel rectangle classifier is at least 4 and in fact it is exactly for as we will see below.\\
	Consider any set of 5 points in $\mathbb{R}^2$. Let $p_1$ be the point with the largest $y$-coordinate and $p_2$ be the point with the smallest $y$-coordinate. $p_3$ has the largest $x$ coordinate and $p_4$ has the smallest. Notice that each pair of points may be identical. Finally, there is a fifth point $q$ that is somewhere in the rectangle spanned by those four points. If we give points $p_1$ to $p_4$ the class 1 and $q$ class 0, then we can't shatter any set of five points implying that VC-dim $\leq 4$.
	\begin{example}
		Let $X = \mathbb{R}$. Define \[H = \{h_{[a,b]}: a\leq b\}\] where \[h_{[a,b]}(x) = \begin{cases}
			1 \; x \in [a,b]\\
			0; \text{ else}
		\end{cases}\]
		Obviously $VC(H) \geq 2$ because there is always an interval that contains two points. Indeed the dimension is exactly 2 because if we have $x_1 < x_2 < x_3$ with classes 1, 0 and 1, there is no classifier that is correct.
	\end{example}
	The class of homogeneous linear classifiers in $\mathbb{R}^d$ \[h_w: x \mapsto sgn(w^Tx)\] has VC-dimension of $d$.
	\subsection{Fundamental Theorem of PAC-learning}
	\begin{theorem}
		Let $H$ be a set of binary classifiers. Then $H$ is PAC-learnable iff the VC dimension of $H$ is finite.
	\end{theorem}
	\begin{theorem}
		Let $H$ be a set of binary classifiers with $VC(H) = d < \infty$. Then $\exists c > 0$ s.t. $\forall \varepsilon, \delta > 0$ it holds that with probability at least $1-\delta$ \[L_D(h_S) \leq \inf_{h \in H} L_D(h) + \sqrt{c\frac{d+\log(\frac{1}{\delta})}{m}}\]
	\end{theorem}
	\begin{definition}
		Let $H$ be a set of binary classifiers. We define the growth function \[\tau: \mathbb{N} \to \mathbb{N}\] with \[\tau(n) = \max\limits_{C\subseteq X, \left|C\right| = m} \left|H|_C\right| = \max\left|\{h|_c: C \to \{0,1\}: h \in H\}\right|\]
	\end{definition}
	\begin{lemma}
		Let $H$ be a set of binary classifiers with $VC(H) = d < \infty$. Then \[\tau(m) \leq \sum_{i=0}^d \binom{m}{i} \; \forall m \in \mathbb{N}\] In particular if \[m \geq d+1\] then $\tau(m) \leq (d+1)\cdot m^{d+1}$.
	\end{lemma}
	\begin{theorem}
		Let $H$ be a set of binary classifiers. Then for every $\delta >0$ with probability of at least $1-\delta$ over the choice $S\sim D^m$ of the training set it holds that \[\sup_{h \in H} \left|L_D(h) - L_S(h)\right| \leq \frac{4+\sqrt{\log(\tau_H(2\cdot m))}}{\delta\sqrt{2\cdot m}}\] 
	\end{theorem}
	\begin{lemma}
		Let $H$ be a set of binary classifiers. Then for every $\delta > 0$ with probability at least $1-\delta$ over the choice $S\sim D^m$ of the training set it holds that \[\sup_{h \in H} \left|L_D(h) - L_S(h)\right| \leq \frac{8+2\sqrt{\log(\tau_H(2\cdot m))}}{\delta \sqrt{m}}\]
	\end{lemma}
	\underline{Note:} The lemma is a weaker bound than the one in above theorem and can thus be ignored.
	\begin{theorem}
		Let $H$ be a set of binary classifiers with the domain set $X$ of VC-dimension $\geq2m$ and a learning algorithm $A$. Then there is a distribution $D$ on $X\times \{0,1\}$ s.t. \begin{enumerate}
			\item $\exists h^* \in H$ with $L_D(h^*) = 0$
			\item with probability $\geq \frac{1}{7}$ and a choice of training set $S$ with $\left|S\right| = m$ we have: \[L_D(A(S)) \geq \frac{1}{8}\]
		\end{enumerate}
	\end{theorem}

	\subsection{VC-dimension of linear classifiers}
	\begin{theorem}
		The class of linear classifiers in $\mathbb{R}^d$ \[H = \{x\mapsto sgn(w^Tx): \; w \in \mathbb{R}^d\}\]
		has VC-dimension $d$.
	\end{theorem}
	\subsection{neural networks}
	For an input vector $x$, weights $w$ and a bias $b$ the output of an artificial neuron is $w^Tx+b$.\\
	A classification network is made up of an input layer, the hidden layer where the neurons are and the output layer. In such a case, the weight vector $w$ would be a weight matrix $W^{(1)}$. The each neuron then has a weight to the output neuron which also has a bias $b^{(2)}$ so the final output is then given by $sgn(W^{(2)}h + b^{(2)})$ where $h$ is the output of the hidden layer. One can notice that this is simply an affine function. Hence neurons are augmented by an activation function, e.g. ReLU$(x) = \max(0,x)$ (=Rectified linear unit). If the activation function is non-linear one can calculate more difficult stuff than with normal affine functions.\\
	Activation functions $\sigma: \mathbb{R} \to \mathbb{R}$ are applied component-wise. It is important to note, that neurons on the same layer must share the same activation function. The necessity of activation functions becomes inherently apparent, when one tries to compute XOR with a neural network. Even though XOR is not separable, one can compute the XOR function using a neural network, if an activation function like ReLU is used.\\
	Modern neural networks consist of many hidden layers, each equipped with ReLU while the output layer usually uses the logistic function or softmax. A logistic function output does not give a simple class but a confidence level instead. This is a difference to the classical classification task. Quite similarly, one can use a softmax function defined by \[z \mapsto \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}\] This gives a distribution over the classes instead.
	\begin{theorem}
		Consider the class $F$ of neural networks with $L-1$ fully connected hidden layers and one output layer. Each hidden layer uses the ReLU function as an activation function whereas the output has $sgn$ as activation function. Let the number of weights be upper bounded by $N$. Then \[VCdim(F) \in \mathcal{O}(NL\cdot \log N)\]
		There is also a lower bound given by \[ VCdim(F) \in \Omega\left(NL\cdot \log\left(\frac{N}{L}\right)\right)\] 
		Using the $\tanh$ activation function, we find \[VCdim(F) \in \Omega(N^2)\]
	\end{theorem}
	\section{stochastic gradient descent}
	\subsection{convexity}
	\begin{definition}[convex set]
		A set $M$ is convex, if $\forall x,y \in M, \; \forall \lambda \in [0,1]$, $$\lambda x + (1-\lambda) y \in M$$
	\end{definition}
	\begin{definition}[convex function]
		A function $f$ is convex if $\forall x_1, x_2$ in the convex preimage of $f$ and $\forall \lambda \in [0,1]$ we have \[f(\lambda x_1 + (1-\lambda) x_2)\leq\lambda f(x_1) + (1-\lambda) f(x_2)\]
	\end{definition}
	There is a relation between convex sets and convex functions.
	\begin{remark}
		Let $C$ be a convex set and $f$ be a function on $C$, then the epigraph $epi(f) = \{(x,y): x \in C, y \geq f(x)\}$ is a convex set iff $f$ is a convex function. 
	\end{remark}
	\subsection{convex optimization}
	\begin{definition}
		Let $K\subseteq \mathbb{R}^n$ be convex, $f: K \to \mathbb{R}$ convex. A convex optimization problem searches for \[\inf_{x\in K} f(x)\]
	\end{definition}
	\begin{remark}
		Any local minimum of a convex function is a global minimum.
	\end{remark}
	\begin{lemma}
		Let $f$ be a doubly differentiable function with compact preimage. The following statements are equivalent
		\begin{itemize}
			\item $f$ is convex
			\item $f'$ is monotonically non-decreasing
			\item $f''$ is non-negative
		\end{itemize}
	\end{lemma}
	\begin{lemma}
		Let $g: \mathbb{R} \to \mathbb{R}$ be convex and let $x \to w^Tx+b$. Then $f: \mathbb{R}^n \to \mathbb{R}$ defined by $f(x) = g(w^Tx+b)$ is convex as well.
	\end{lemma}
	As a consequence from above lemma, $w \mapsto \log(1+e^{-yw^tx})$ is convex.
	\begin{lemma}
		Let $C\subset \mathbb{R}^n$ be a convex set, $f_1,...,f_n$ are convex functions from $C$ to $\mathbb{R}$ and weights $w_1,...,w_n \geq 0$, then \[f= \sum_{i=1}^n w_if_i\] is convex.
	\end{lemma}
	As a consequence from this lemma, the logistic loss function is convex.
	\begin{lemma}
		Let $I$ be some index set and $C$ be a convex set. Let $f_i: C \to \mathbb{R}$ be a family of convex functions. Then \[f:x\mapsto \sup_{i\in I} f_i(x)\] is a convex function.
	\end{lemma}
	\subsection{Strong convexity}
	Let $f: K \to \mathbb{R}$ where $K$ is convex, $\mu>0$ $f$ is $\mu$-strongly convex if \[\frac{\mu}{2}\lambda(1-\lambda)||x-y||_2^2 + f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y) \; \forall x,y \in K, \; \lambda \in [0,1]\]
	\begin{lemma}
		The map $x\mapsto ||x||_2^2$ is 2-strongly convex.
	\end{lemma}
	\begin{lemma}
		Let $f:K \to \mathbb{R}$ be convex and $g: K \mapsto \mathbb{R}$ $\mu$-strongly convex then \begin{enumerate}
			\item $f+g$ is also strongly convex
			\item $c\cdot g$ is $c\cdot \mu$-strongly convex for some $c > 0$.
		\end{enumerate}
	\end{lemma}
	\begin{lemma}
		Let $f:K \to \mathbb{R}$ be differentiable, $K \subseteq \mathbb{R}^n$ convex and open. Then $f$ is (strongly) convex iff \[f(y) \geq f(x) + \nabla f(x)^T(y-x) (+\frac{\mu}{2}||x-y||_2^2)\]
	\end{lemma}
	\subsection{gradient descent}
	Suppose $\overline{x}$ is the output of the gradient descent to minimize a convex function $f$ where the global minimum is located at $x^*$. Then \[f(\overline{x}) - f(x^*) = \varepsilon\] where $\varepsilon \sim e^{-t}$. However in Machine Learning, where large datasets are regularly used, computing one gradient can take a very large amount of resources, hence we need another solution.\\
	To compute the direction in which to iterate, pick $(x,y) \in S$ at random and use $\nabla L_{(x,y)}(w)$ which is in expectation the same value as the gradient in the normal gradient descent algorithm. This is called the stochastic gradient descent. Instead of computing \[\frac{1}{\left|S\right|} \sum_{(x,y) \in S} \nabla L_{(x,y)}(w)\] as in the classical gradient descent, we compute \[\frac{1}{\left|M\right|} \sum_{(x,y) \in M} \nabla L_{(x,y)}(w)\] for a small subset $M$ of $S$.
	\subsection{convergence of stochastic gradient descent}
	We make the following assumptions: \begin{itemize}
		\item the loss function is differentiable
		\item the loss function is $\mu$-strongly convex
		\item $\sup_{w} ||\nabla L_{(x,y)}(w)||^2 \leq B$ for all $(x,y) \in S$.
	\end{itemize}
	The learning rate is defined by \[\eta_1 \geq \eta_2 \geq ... > 0 \text{ s.t. } \sum_{t=1}^\infty \eta_t = \infty \text{ but } \sum_{t=1}^\infty \eta_t^2 < \infty\]
	\begin{theorem}
		Under these assumptions \[\mathbb{E}_{1,...,t} [||w^{(t+1)} - w^*||_2^2] \to 0 \text{ as } t \to \infty\] where $w^*$ is the global minimum.
	\end{theorem}
	\begin{lemma}
		Define $\varepsilon = \mathbb{E}_{1,...,t-1} [||w^{(t)} - w^*||_2^2]$. Then \[\varepsilon_t \leq \varepsilon_{t-1}(1-\eta_{t-1}\mu) + \eta_{t-1}^2B\]
	\end{lemma}
	\begin{lemma}
		\[\varepsilon_{T'+k+1}\leq \varepsilon_{T'}\prod_{t=T'}^{T'+k}(1-\eta_t\mu) + \sum_{t=T'}^{T'+k}\eta_t^2B\]
	\end{lemma}
	\section{neural networks}
	\subsection{back propagation}
	We now introduce a method to train a neural network with SGD.\\
	For that we define a matrix $W$ which collects all weights and biases. Assume we have $K$ layers. At the input we have $x \in \mathbb{R}^{n_0}$. The first layer produces \[g^{(1)}(x) = W^{(1)}x+b^{(1)}\] but the output of the first hidden layer is \[f^{(1)}(x) = \sigma_1(g^{(1)}(x))\] where $\sigma$ is the activation function, which is ReLU for the hidden layer and the logistic loss function for the output. This is easily generalized for the $l$-th hidden layer. We generally prefer the notation $g^{(k)} = w^{(k)}f^{(k-1)} + b^{(k)}$ because the $x$-term is always the same, so we omit it. We generally define the dimension of the $k$-th layer by $n_k$. In particular $n_K = 1$. We need to compute the values of \[\frac{\partial L_{(x,y)}}{\partial w^{(k)}_{i,h}} \text{ and } \frac{\partial L_{(x,y)}}{\partial b_j^{(k)}}\]
	Starting at the output, we have \[f_1^{(K)} = \sigma_K(g^{(K)}_1) = \sigma_K(W^{(K)}f^{(K-1)}+b_1^K)\]
	Also, $L_{(x,y)} = l(y,f_1^{(K)})$. We find \[\frac{\partial L_{(x,y)}}{\partial b_1^{(k)}} = \frac{\partial L_{(x,y)}}{\partial f_1^{(K)}} \cdot \frac{\partial f_1^{(K)}}{\partial g_1^{(K)}}\cdot \frac{\partial g_1^{(K)}}{\partial b_1^{(k)}}\] and \[\frac{\partial L_{(x,y)}}{\partial w^{(k)}_{1,h}} = \frac{\partial L_{(x,y)}}{\partial f_1^{(K)}} \cdot \frac{\partial f_1^{(K)}}{\partial g_1^{(K)}}\cdot \frac{\partial g_1^{(K)}}{\partial w^{(k)}_{1,h}}\] where the two common terms in the calculations are defined as $\delta_1^{(K)}$. In particular, we find \[\frac{\partial L_{(x,y)}}{\partial b_1^{(k)}} = \delta_1^{(K)}\] and \[\frac{\partial L_{(x,y)}}{\partial w^{(k)}_{1,h}} = \delta_1^{(K)}\cdot f_h^{(K-1)}\]
	We know concentrate on the $k$-th layer for an arbitrary $k$. Define $$\delta^{(k)} = \nabla_{g^{(k)}} L_{(x,y)} = \left(\frac{\partial L_{(x,y)}}{\partial g_1^{(k)}}, \dots , \frac{\partial L_{(x,y)}}{\partial g_{n_k}^{(k)}}\right)$$
	This the shows \[\frac{\partial L_{(x,y)}}{\partial b_h^{(k)}} = \frac{\partial L_{(x,y)}}{\partial g_h^{(k)}}\cdot \frac{\partial g_h^{(k)}}{\partial b_h^{(k)}} = \delta_h^{(k)}\] and \[\frac{\partial L{(x,y)}}{\partial w_{h,i}^{(k)}} = \frac{\partial L_{(x,y)}}{\partial wg_h^{(k)}} \cdot \frac{\partial g_h^{(k)}}{\partial w_{h,i}^{(k)}} = \delta_h^{(k)}f_i^{(k-1)}\]
	We hence want to figure out how we can compute the $\delta$. \[\delta_h^{(k)} = \frac{\partial L_{(x,y)}}{\partial g_h^{(k)}} = \sum_{i=1}^{n_{k+1}}\frac{\partial L_{(x,y)}}{\partial g_i^{(k+1)}}\cdot \frac{\partial g_i^{(k+1)}}{\partial g_h^{(k)}} = \sum_{i=1}^{n_{k+1}} \delta_i^{(k+1)}\cdot \frac{\partial g_i^{(k+1)}}{\partial g_h^{(k)}}\]
	But now $g_i^{(k+1)} = \left(W^{(k+1)}f^{(k)}\right)_i + b_i^{(k+1)}$ and $f^{(k)} = \sigma_k(g^{(k)})$. For the calculation of $\delta$ we need \[\frac{\partial g_i^{(k+1)}}{\partial g_h^{(k)}} = \frac{\partial g_i^{(k+1)}}{\partial f_h^{(k)}} \frac{\partial f_h^{(k)}}{\partial g_h^{(k)}} = w_{i,h}^{(k+1)}\cdot \sigma_k(f_h^{(k)})\] which implies that \[\delta_h^{(k)} = \sum_{i=1}^{n_{k+1}} \delta_i^{(k+1)}\cdot w_{i,h}^{(k+1)}\cdot \sigma_k'(f_h^{(k)}) = \left(W^{(k+1)T}\cdot \delta^{(k+1)}\right)_h\cdot \sigma_k'(f_h^{(k)})\]
	Generally, we can write \[\delta^{(k)} = \left(W^{(k+1)T}\cdot \delta^{(k+1)}\right) \odot \sigma_k^*(f^{(k)})\] where $\odot$ is the Hadamard matrix product defined by \[(A\odot B)_{ij} = A_{ij}\cdot B_{ij}\]
	\subsection{Loss functions for neural networks}
	The following loss function are options \begin{itemize}
		\item square loss: $L = \frac{1}{\left|S\right|} \sum_{(x,y) \in S} \left|y-f_1^{(K)}(x)\right|^2$
		\item cross entropy loss: $L = -\frac{1}{\left|S\right|} \sum_{(x,y) \in S} y\cdot \log(f_1^{(K)}(x)) + (1-y)\log(1-f_1^{(K)}(x))$
	\end{itemize}
	Both are non-negative. One can show, that the second function is better for fast learning using the back propagation algorithm.
	\begin{definition}[soft max]
		Soft max is an activation function for the output layer that does not return a fixed class but rather a probability distribution over all possible classes. It is defined by the mapping of $z = (z_1,...,z_k) \in \mathbb{R}^k$ to $[0,1]^k$ under \[z \mapsto \left(\frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}\right)\]
		Usually one should use log-likelihood loss \[L = - \frac{1}{\left|S\right|} \sum_{(x,y) \in S} \sum_{j=1}^{n_j} \log f_j^{(K)}(x)\] 
	\end{definition}
\end{document}